########################
# INPUT
########################
input {
  file {
    path => "C:/dfir/output/parsed_*_skadi/LNK/*.csv"
    mode => "read"
    start_position => "beginning"
    sincedb_path => "NUL"

    file_completed_action => "log"
    file_completed_log_path => "C:/dfir/output/processed_all.log"
	#file_chunk_size => 524288

    codec => plain { charset => "UTF-8" }
  }
}

########################
# FILTER
########################
filter {

  # ---------------------------------
  # Clean Windows CRLF issues
  # ---------------------------------
  mutate {
    gsub => [
      "message", "\r", ""
    ]
  }

  # ---------------------------------
  # Drop empty lines
  # ---------------------------------
  if [message] =~ /^\s*$/ {
    drop { }
  }

  # ---------------------------------
  # CSV PARSER (NO quote_char!)
  # ---------------------------------
  csv {
    source => "message"
    separator => ","
    skip_header => true
    autogenerate_column_names => false
    columns => [
      "SourceFile",
      "SourceCreated",
      "SourceModified",
      "SourceAccessed",
      "TargetCreated",
      "TargetModified",
      "TargetAccessed",
      "FileSize",
      "RelativePath",
      "WorkingDirectory",
      "FileAttributes",
      "HeaderFlags",
      "DriveType",
      "VolumeSerialNumber",
      "VolumeLabel",
      "LocalPath",
      "NetworkPath",
      "CommonPath",
      "Arguments",
      "TargetIDAbsolutePath",
      "TargetMFTEntryNumber",
      "TargetMFTSequenceNumber",
      "MachineID",
      "MachineMACAddress",
      "MACVendor",
      "TrackerCreatedOn",
      "ExtraBlocksPresent"
    ]
  }

  # ---------------------------------
  # HARD STOP IF CSV FAILS
  # ---------------------------------
  if "_csvparsefailure" in [tags] {
    mutate { add_tag => ["lnk_csv_failed"] }
    drop { }
  }

  # ---------------------------------
  # Type conversions
  # ---------------------------------
  mutate {
    convert => {
      "FileSize" => "integer"
      "TargetMFTEntryNumber" => "integer"
      "TargetMFTSequenceNumber" => "integer"
    }
  }

  # ---------------------------------
  # Date normalization (safe for blanks)
  # ---------------------------------
  date {
    match => ["SourceCreated", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
    target => "source.created"
  }

  date {
    match => ["SourceModified", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
    target => "source.modified"
  }

  date {
    match => ["SourceAccessed", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
    target => "source.accessed"
  }

  date {
    match => ["TargetCreated", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
    target => "file.created"
  }

  date {
    match => ["TargetModified", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
    target => "file.modified"
  }

  date {
    match => ["TargetAccessed", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
    target => "file.accessed"
  }

  # ---------------------------------
  # DFIR Metadata
  # ---------------------------------
  mutate {
    add_field => {
      "artifact_type" => "lnk"
      "dfir_source"   => "windows_lnk"
    }
  }

  # ---------------------------------
  # Extract LNK filename
  # ---------------------------------
  if [SourceFile] {
    grok {
      match => {
        "SourceFile" => "%{GREEDYDATA}[\\/](?<lnk_filename>[^\\/]+)$"
      }
    }
  }
  date {
    match => ["SourceModified", "yyyy-MM-dd HH:mm:ss"]
    target => "@timestamp"
  }
  # ---------------------------------
  # Cleanup
  # ---------------------------------
  mutate {
    remove_field => [
      "message",
      "SourceCreated",
      "SourceModified",
      "SourceAccessed",
      "TargetCreated",
      "TargetModified",
      "TargetAccessed"
    ]
  }
}

########################
# OUTPUT
########################
output {

  elasticsearch {
    hosts => ["https://13.233.58.209:9200"]
    user => "elastic"
    password => "wsQcbfozMJF2BEnljEAq"

    ssl_enabled => true
    ssl_verification_mode => "none"
    index => "dfir-lnk"
  }
}
