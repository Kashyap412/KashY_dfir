########################
# INPUT
########################
input {
  file {
    path => "C:/dfir/output/parsed_*_skadi/Prefetch/*.csv"
    mode => "read"
    start_position => "beginning"
    sincedb_path => "NUL"

    file_completed_action => "log"
    file_completed_log_path => "C:/dfir/output/processed_all.log"
	#file_chunk_size => 524288

    codec => plain { charset => "UTF-8" }
  }
}

########################
# FILTER
########################
filter {

  # ---------------------------------
  # Clean Windows CRLF
  # ---------------------------------
  mutate {
    gsub => ["message", "\r", ""]
  }

  # ---------------------------------
  # Drop empty lines
  # ---------------------------------
  if [message] =~ /^\s*$/ {
    drop { }
  }

  # ---------------------------------
  # Identify Prefetch CSV type by filename
  # ---------------------------------
  if [log][file][path] =~ /_PECmd_Output_Timeline\.csv$/ {

    #############################
    # TIMELINE CSV
    #############################
    csv {
      source => "message"
      separator => ","
      skip_header => true
      autogenerate_column_names => false
      columns => [
        "RunTime",
        "ExecutableName"
      ]
    }

    if "_csvparsefailure" in [tags] {
      drop { }
    }

    date {
      match => ["RunTime", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "@timestamp"
    }

    mutate {
      add_field => {
        "artifact_type" => "prefetch"
        "prefetch_type" => "timeline"
        "dfir_source"   => "windows_prefetch"
      }
    }

  } else if [log][file][path] =~ /_PECmd_Output\.csv$/ {

    #############################
    # MAIN PREFETCH CSV
    #############################
    csv {
      source => "message"
      separator => ","
      skip_header => true
      autogenerate_column_names => false
      columns => [
        "Note",
        "SourceFilename",
        "SourceCreated",
        "SourceModified",
        "SourceAccessed",
        "ExecutableName",
        "Hash",
        "Size",
        "Version",
        "RunCount",
        "LastRun",
        "PreviousRun0",
        "PreviousRun1",
        "PreviousRun2",
        "PreviousRun3",
        "PreviousRun4",
        "PreviousRun5",
        "PreviousRun6",
        "Volume0Name",
        "Volume0Serial",
        "Volume0Created",
        "Volume1Name",
        "Volume1Serial",
        "Volume1Created",
        "Directories",
        "FilesLoaded",
        "ParsingError"
      ]
    }

    if "_csvparsefailure" in [tags] {
      drop { }
    }

    # ---------------------------------
    # Type conversions
    # ---------------------------------
    mutate {
      convert => {
        "Size"     => "integer"
        "RunCount" => "integer"
      }
    }

    # ---------------------------------
    # Date parsing (safe for blanks)
    # ---------------------------------
    date {
      match => ["SourceCreated", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "source.created"
    }

    date {
      match => ["SourceModified", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "source.modified"
    }

    date {
      match => ["SourceAccessed", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "source.accessed"
    }

    date {
      match => ["LastRun", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "process.last_run"
    }

    date {
      match => ["PreviousRun0", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "process.previous_run.0"
    }

    date {
      match => ["PreviousRun1", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "process.previous_run.1"
    }

    date {
      match => ["PreviousRun2", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "process.previous_run.2"
    }

    date {
      match => ["PreviousRun3", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "process.previous_run.3"
    }

    date {
      match => ["PreviousRun4", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "process.previous_run.4"
    }

    date {
      match => ["PreviousRun5", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "process.previous_run.5"
    }

    date {
      match => ["PreviousRun6", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
      target => "process.previous_run.6"
    }

    # ---------------------------------
    # DFIR Metadata
    # ---------------------------------
    mutate {
      add_field => {
        "artifact_type" => "prefetch"
        "prefetch_type" => "main"
        "dfir_source"   => "windows_prefetch"
      }
    }

  } else {
    drop { }
  }
  date {
    match => ["SourceModified", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
    target => "@timestamp"
  }
  # ---------------------------------
  # Extract Prefetch filename
  # ---------------------------------
  if [SourceFilename] {
    grok {
      match => {
        "SourceFilename" => "%{GREEDYDATA}[\\/](?<prefetch_filename>[^\\/]+)$"
      }
    }
  }

  # ---------------------------------
  # Cleanup
  # ---------------------------------
  mutate {
    remove_field => [
      "message",
      "SourceCreated",
      "SourceModified",
      "SourceAccessed",
      "LastRun",
      "PreviousRun0",
      "PreviousRun1",
      "PreviousRun2",
      "PreviousRun3",
      "PreviousRun4",
      "PreviousRun5",
      "PreviousRun6"
    ]
  }
}

########################
# OUTPUT
########################
output {

  elasticsearch {
    hosts => ["https://13.233.58.209:9200"]
    user => "elastic"
    password => "wsQcbfozMJF2BEnljEAq"

    ssl_enabled => true
    ssl_verification_mode => "none"
    index => "dfir-prefetch"
  }
}
